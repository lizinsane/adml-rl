{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Lab Solutions - Multi-Armed Bandits\n",
    "\n",
    "\n",
    "This notebook contains complete solutions with detailed explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Bandit Environment - Solution\n",
    "\n",
    "**Key Concepts:**\n",
    "- Each arm has a Gaussian (normal) reward distribution\n",
    "- The mean determines the expected reward, std controls variance\n",
    "- We use `np.random.randn()` for standard normal and scale it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    \"\"\"\n",
    "    Multi-Armed Bandit environment.\n",
    "    \n",
    "    Each arm has a Gaussian reward distribution with a specific mean and standard deviation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, means: List[float] = None, std: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize the bandit.\n",
    "        \n",
    "        Args:\n",
    "            n_arms: Number of arms\n",
    "            means: List of mean rewards for each arm. If None, randomly generate.\n",
    "            std: Standard deviation of reward distributions\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.std = std\n",
    "        \n",
    "        if means is None:\n",
    "            # Generate random means from a standard normal distribution\n",
    "            # This ensures diversity in arm quality\n",
    "            self.means = np.random.randn(n_arms)\n",
    "        else:\n",
    "            self.means = np.array(means)\n",
    "        \n",
    "        # Track which arm is optimal (highest expected reward)\n",
    "        self.optimal_arm = np.argmax(self.means)\n",
    "    \n",
    "    def pull(self, arm: int) -> float:\n",
    "        \"\"\"\n",
    "        Pull an arm and receive a reward.\n",
    "        \n",
    "        The reward is sampled from N(mean[arm], std^2)\n",
    "        \n",
    "        Args:\n",
    "            arm: Index of the arm to pull (0 to n_arms-1)\n",
    "            \n",
    "        Returns:\n",
    "            reward: Reward sampled from the arm's distribution\n",
    "        \"\"\"\n",
    "        # Sample from N(mean, std^2) = mean + std * N(0, 1)\n",
    "        reward = self.means[arm] + self.std * np.random.randn()\n",
    "        return reward\n",
    "    \n",
    "    def get_optimal_value(self) -> float:\n",
    "        \"\"\"Return the expected reward of the optimal arm.\"\"\"\n",
    "        return self.means[self.optimal_arm]\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "bandit = MultiArmedBandit(n_arms=5, means=[0.5, 1.2, 0.8, 2.1, 1.5])\n",
    "print(f\"Bandit means: {bandit.means}\")\n",
    "print(f\"Optimal arm: {bandit.optimal_arm} with mean {bandit.get_optimal_value():.2f}\")\n",
    "print(f\"\\nSample rewards from arm {bandit.optimal_arm}: \", [f\"{bandit.pull(bandit.optimal_arm):.2f}\" for _ in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "**Why Gaussian distributions?**\n",
    "- They're realistic: many real-world rewards have natural variation around a mean\n",
    "- They're unbounded: rewards can theoretically be any value\n",
    "- They're mathematically convenient\n",
    "\n",
    "**Random mean initialization:**\n",
    "- Ensures different problem instances have different optimal arms\n",
    "- Tests that our agent can find the best arm regardless of which one it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Epsilon-Greedy Agent - Solution\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Exploration (ε):** Try random actions to learn about them\n",
    "- **Exploitation (1-ε):** Use current knowledge to get best reward\n",
    "- **Incremental update:** Efficiently compute running average without storing all rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent:\n",
    "    \"\"\"\n",
    "    Agent using epsilon-greedy action selection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, epsilon: float = 0.1, initial_value: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize the agent.\n",
    "        \n",
    "        Args:\n",
    "            n_arms: Number of arms\n",
    "            epsilon: Exploration probability\n",
    "            initial_value: Initial Q-value estimates for all arms\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Q-values: estimated value of each arm\n",
    "        self.Q = np.ones(n_arms) * initial_value\n",
    "        \n",
    "        # Count how many times each arm was pulled\n",
    "        self.N = np.zeros(n_arms)\n",
    "    \n",
    "    def select_action(self) -> int:\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy strategy.\n",
    "        \n",
    "        With probability epsilon: explore (random action)\n",
    "        With probability 1-epsilon: exploit (best known action)\n",
    "        \n",
    "        Returns:\n",
    "            action: Selected arm index\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: choose random arm\n",
    "            action = np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            # Exploit: choose arm with highest Q-value\n",
    "            action = np.argmax(self.Q)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update(self, action: int, reward: float):\n",
    "        \"\"\"\n",
    "        Update Q-value estimate after observing a reward.\n",
    "        \n",
    "        Uses incremental mean formula:\n",
    "        NewEstimate = OldEstimate + StepSize * (Target - OldEstimate)\n",
    "        \n",
    "        Where StepSize = 1/N gives us the sample average.\n",
    "        \n",
    "        Args:\n",
    "            action: The arm that was pulled\n",
    "            reward: The observed reward\n",
    "        \"\"\"\n",
    "        # Increment count for this arm\n",
    "        self.N[action] += 1\n",
    "        \n",
    "        # Update Q-value using incremental mean formula\n",
    "        # Q_new = Q_old + (1/N) * (reward - Q_old)\n",
    "        # This is equivalent to: Q_new = (Q_old * (N-1) + reward) / N\n",
    "        # but more numerically stable and efficient\n",
    "        self.Q[action] += (reward - self.Q[action]) / self.N[action]\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "agent = EpsilonGreedyAgent(n_arms=5, epsilon=0.1)\n",
    "print(f\"Initial Q-values: {agent.Q}\")\n",
    "\n",
    "# Simulate pulls from a fixed arm to verify learning\n",
    "test_arm = 2\n",
    "true_mean = 1.5\n",
    "for _ in range(100):\n",
    "    reward = true_mean + np.random.randn()  # N(1.5, 1)\n",
    "    agent.update(test_arm, reward)\n",
    "\n",
    "print(f\"\\nAfter 100 pulls of arm {test_arm} (true mean = {true_mean}):\")\n",
    "print(f\"Learned Q-value for arm {test_arm}: {agent.Q[test_arm]:.3f}\")\n",
    "print(f\"Number of pulls: {agent.N[test_arm]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "**Why the incremental update formula?**\n",
    "\n",
    "Instead of storing all rewards and computing:\n",
    "```python\n",
    "Q[a] = mean(all_rewards_for_arm_a)\n",
    "```\n",
    "\n",
    "We use:\n",
    "```python\n",
    "Q[a] = Q[a] + (reward - Q[a]) / N[a]\n",
    "```\n",
    "\n",
    "This is:\n",
    "1. **Memory efficient:** Only store one value per arm\n",
    "2. **Computationally efficient:** O(1) update instead of O(N)\n",
    "3. **Mathematically equivalent:** Gives the same sample average\n",
    "\n",
    "**Understanding the update:**\n",
    "- If `reward > Q[a]`, we increase Q[a]\n",
    "- If `reward < Q[a]`, we decrease Q[a]\n",
    "- The step size `1/N[a]` decreases over time → estimates stabilize\n",
    "\n",
    "**Epsilon value choice:**\n",
    "- ε = 0.1 means 10% exploration, 90% exploitation\n",
    "- This is a common default that works well in many problems\n",
    "- Too small ε → might miss the best arm\n",
    "- Too large ε → waste time on suboptimal arms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: UCB Agent - Solution\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Optimism in the face of uncertainty:** Arms we've tried less are more uncertain\n",
    "- **Bonus term:** $c\\sqrt{\\frac{\\ln t}{N_t(a)}}$ gives higher priority to less-tried arms\n",
    "- **Automatic exploration:** No need to manually set exploration rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent:\n",
    "    \"\"\"\n",
    "    Agent using Upper Confidence Bound (UCB) action selection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, c: float = 2.0, initial_value: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize the agent.\n",
    "        \n",
    "        Args:\n",
    "            n_arms: Number of arms\n",
    "            c: Exploration parameter (typically 2.0)\n",
    "            initial_value: Initial Q-value estimates\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.c = c\n",
    "        \n",
    "        self.Q = np.ones(n_arms) * initial_value\n",
    "        self.N = np.zeros(n_arms)\n",
    "        self.t = 0  # Total time steps\n",
    "    \n",
    "    def select_action(self) -> int:\n",
    "        \"\"\"\n",
    "        Select an action using UCB.\n",
    "        \n",
    "        UCB formula: A_t = argmax[Q(a) + c * sqrt(ln(t) / N(a))]\n",
    "        \n",
    "        Returns:\n",
    "            action: Selected arm index\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # For arms that haven't been pulled yet, give them infinite value\n",
    "        # This ensures all arms are tried at least once\n",
    "        if np.any(self.N == 0):\n",
    "            # Return the first untried arm\n",
    "            action = np.argmax(self.N == 0)\n",
    "        else:\n",
    "            # Calculate UCB values for all arms\n",
    "            # UCB = Q + c * sqrt(ln(t) / N)\n",
    "            ucb_values = self.Q + self.c * np.sqrt(np.log(self.t) / self.N)\n",
    "            action = np.argmax(ucb_values)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update(self, action: int, reward: float):\n",
    "        \"\"\"\n",
    "        Update Q-value estimate.\n",
    "        \n",
    "        Same incremental update as epsilon-greedy.\n",
    "        \n",
    "        Args:\n",
    "            action: The arm that was pulled\n",
    "            reward: The observed reward\n",
    "        \"\"\"\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] += (reward - self.Q[action]) / self.N[action]\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "agent = UCBAgent(n_arms=5, c=2.0)\n",
    "bandit = MultiArmedBandit(n_arms=5, means=[0.5, 1.2, 0.8, 2.1, 1.5])\n",
    "\n",
    "print(\"Initial state:\")\n",
    "print(f\"Q-values: {agent.Q}\")\n",
    "print(f\"\\nFirst {agent.n_arms} actions (should try each arm once):\")\n",
    "\n",
    "for step in range(5):\n",
    "    action = agent.select_action()\n",
    "    reward = bandit.pull(action)\n",
    "    agent.update(action, reward)\n",
    "    print(f\"Step {step+1}: Selected arm {action}, reward = {reward:.2f}\")\n",
    "\n",
    "print(f\"\\nAfter initial exploration:\")\n",
    "print(f\"Q-values: {agent.Q}\")\n",
    "print(f\"Arm counts: {agent.N}\")\n",
    "\n",
    "# Continue for more steps\n",
    "for _ in range(20):\n",
    "    action = agent.select_action()\n",
    "    reward = bandit.pull(action)\n",
    "    agent.update(action, reward)\n",
    "\n",
    "print(f\"\\nAfter 25 total steps:\")\n",
    "print(f\"Q-values: {agent.Q}\")\n",
    "print(f\"True means: {bandit.means}\")\n",
    "print(f\"Arm counts: {agent.N}\")\n",
    "print(f\"\\nMost selected arm: {np.argmax(agent.N)} (optimal is {bandit.optimal_arm})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "**Understanding the UCB formula:**\n",
    "\n",
    "$$UCB(a) = Q(a) + c\\sqrt{\\frac{\\ln t}{N(a)}}$$\n",
    "\n",
    "- **$Q(a)$:** Current estimate (exploitation term)\n",
    "- **$c$:** Exploration parameter (controls exploration degree)\n",
    "- **$\\sqrt{\\frac{\\ln t}{N(a)}}$:** Uncertainty bonus (exploration term)\n",
    "\n",
    "**Why this works:**\n",
    "1. The bonus term is large when $N(a)$ is small (arm rarely tried)\n",
    "2. The bonus term grows with $t$ (more total experience → more confidence to explore)\n",
    "3. As we pull an arm more, $N(a)$ increases → bonus decreases → focus on exploitation\n",
    "\n",
    "**Key advantages over epsilon-greedy:**\n",
    "- **Adaptive exploration:** Automatically reduces exploration over time\n",
    "- **Directed exploration:** Focuses on uncertain arms, not random arms\n",
    "- **Theoretical guarantees:** Provably finds optimal arm with logarithmic regret\n",
    "\n",
    "**Initial exploration:**\n",
    "- We explicitly check if any arm has $N(a) = 0$\n",
    "- This avoids division by zero\n",
    "- Ensures all arms are tried at least once before UCB selection kicks in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Experiment Runner - Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(agent_class, agent_params: dict, bandit: MultiArmedBandit, \n",
    "                   n_steps: int = 1000) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run a single experiment with a given agent and bandit.\n",
    "    \n",
    "    Args:\n",
    "        agent_class: The agent class to use (EpsilonGreedyAgent or UCBAgent)\n",
    "        agent_params: Dictionary of parameters to pass to agent constructor\n",
    "        bandit: The bandit environment\n",
    "        n_steps: Number of steps to run\n",
    "        \n",
    "    Returns:\n",
    "        rewards: Array of rewards obtained at each step\n",
    "        optimal_actions: Binary array indicating if optimal action was taken\n",
    "        q_values_final: Final Q-value estimates\n",
    "    \"\"\"\n",
    "    # Create the agent\n",
    "    agent = agent_class(**agent_params)\n",
    "    \n",
    "    # Arrays to store results\n",
    "    rewards = np.zeros(n_steps)\n",
    "    optimal_actions = np.zeros(n_steps)\n",
    "    \n",
    "    # Run the experiment\n",
    "    for step in range(n_steps):\n",
    "        # Select action\n",
    "        action = agent.select_action()\n",
    "        \n",
    "        # Pull the bandit arm\n",
    "        reward = bandit.pull(action)\n",
    "        \n",
    "        # Update the agent\n",
    "        agent.update(action, reward)\n",
    "        \n",
    "        # Record metrics\n",
    "        rewards[step] = reward\n",
    "        optimal_actions[step] = 1 if action == bandit.optimal_arm else 0\n",
    "    \n",
    "    return rewards, optimal_actions, agent.Q\n",
    "\n",
    "\n",
    "def run_multiple_experiments(agent_class, agent_params: dict, n_experiments: int = 100, \n",
    "                            n_steps: int = 1000, n_arms: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run multiple experiments and average the results.\n",
    "    \n",
    "    Args:\n",
    "        agent_class: The agent class to use\n",
    "        agent_params: Parameters for the agent\n",
    "        n_experiments: Number of experiments to run\n",
    "        n_steps: Steps per experiment\n",
    "        n_arms: Number of arms in the bandit\n",
    "        \n",
    "    Returns:\n",
    "        avg_rewards: Average rewards across experiments\n",
    "        avg_optimal: Average percentage of optimal actions\n",
    "    \"\"\"\n",
    "    all_rewards = np.zeros((n_experiments, n_steps))\n",
    "    all_optimal = np.zeros((n_experiments, n_steps))\n",
    "    \n",
    "    for i in range(n_experiments):\n",
    "        # Create a new bandit for each experiment\n",
    "        # This tests robustness across different problem instances\n",
    "        bandit = MultiArmedBandit(n_arms=n_arms)\n",
    "        \n",
    "        rewards, optimal_actions, _ = run_experiment(agent_class, agent_params, bandit, n_steps)\n",
    "        \n",
    "        all_rewards[i] = rewards\n",
    "        all_optimal[i] = optimal_actions\n",
    "    \n",
    "    return np.mean(all_rewards, axis=0), np.mean(all_optimal, axis=0)\n",
    "\n",
    "\n",
    "# Test with a simple case\n",
    "test_bandit = MultiArmedBandit(n_arms=5, means=[0.5, 1.2, 0.8, 2.1, 1.5])\n",
    "rewards, optimal, final_q = run_experiment(\n",
    "    EpsilonGreedyAgent, \n",
    "    {'n_arms': 5, 'epsilon': 0.1}, \n",
    "    test_bandit, \n",
    "    n_steps=1000\n",
    ")\n",
    "\n",
    "print(f\"Test run completed: {len(rewards)} rewards collected\")\n",
    "print(f\"Average reward: {np.mean(rewards):.3f}\")\n",
    "print(f\"Optimal action rate: {np.mean(optimal)*100:.1f}%\")\n",
    "print(f\"\\nFinal Q-values: {final_q}\")\n",
    "print(f\"True means: {test_bandit.means}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "**Why run multiple experiments?**\n",
    "- Each experiment has randomness (in bandit rewards and agent's random exploration)\n",
    "- Averaging removes this noise and shows true performance\n",
    "- We use different bandit instances to test generalization\n",
    "\n",
    "**Metrics we track:**\n",
    "1. **Rewards:** Direct measure of performance\n",
    "2. **Optimal action rate:** How often we pick the best arm (regardless of reward)\n",
    "3. **Final Q-values:** Shows if agent learned true arm values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Compare Epsilon-Greedy with Different Epsilon Values - Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for different epsilon values\n",
    "epsilon_values = [0, 0.01, 0.1, 0.3]\n",
    "n_experiments = 100\n",
    "n_steps = 1000\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Running experiments for different epsilon values...\")\n",
    "for eps in epsilon_values:\n",
    "    print(f\"  ε = {eps}...\")\n",
    "    avg_rewards, avg_optimal = run_multiple_experiments(\n",
    "        EpsilonGreedyAgent,\n",
    "        {'n_arms': 10, 'epsilon': eps},\n",
    "        n_experiments=n_experiments,\n",
    "        n_steps=n_steps,\n",
    "        n_arms=10\n",
    "    )\n",
    "    results[eps] = {'rewards': avg_rewards, 'optimal': avg_optimal}\n",
    "\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Average reward over time\n",
    "plt.subplot(1, 2, 1)\n",
    "for eps in epsilon_values:\n",
    "    plt.plot(results[eps]['rewards'], label=f'ε = {eps}', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Steps', fontsize=12)\n",
    "plt.ylabel('Average Reward', fontsize=12)\n",
    "plt.title('Average Reward vs Steps (Different Epsilon Values)', fontsize=13)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Optimal action percentage\n",
    "plt.subplot(1, 2, 2)\n",
    "for eps in epsilon_values:\n",
    "    plt.plot(results[eps]['optimal'] * 100, label=f'ε = {eps}', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Steps', fontsize=12)\n",
    "plt.ylabel('% Optimal Action', fontsize=12)\n",
    "plt.title('Optimal Action Selection Rate (Different Epsilon Values)', fontsize=13)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics (last 100 steps):\")\n",
    "print(\"-\" * 60)\n",
    "for eps in epsilon_values:\n",
    "    final_reward = np.mean(results[eps]['rewards'][-100:])\n",
    "    final_optimal = np.mean(results[eps]['optimal'][-100:]) * 100\n",
    "    print(f\"ε = {eps:4.2f}  |  Avg Reward: {final_reward:5.3f}  |  Optimal: {final_optimal:5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Results:\n",
    "\n",
    "**Expected observations:**\n",
    "\n",
    "1. **ε = 0 (Pure Greedy):**\n",
    "   - **Problem:** Gets stuck on first arm that seems good\n",
    "   - **Initial performance:** Can be good if lucky with initial pulls\n",
    "   - **Long-term:** Usually converges to suboptimal arm\n",
    "   - **Optimal action rate:** Low, often around 10-30%\n",
    "\n",
    "2. **ε = 0.01 (Very low exploration):**\n",
    "   - **Initial learning:** Slow, as it rarely explores\n",
    "   - **Long-term:** Better than greedy, but may still miss optimal\n",
    "   - **Optimal action rate:** Moderate, around 40-60%\n",
    "\n",
    "3. **ε = 0.1 (Balanced):**\n",
    "   - **Initial learning:** Fast discovery of good arms\n",
    "   - **Long-term:** Strong performance\n",
    "   - **Optimal action rate:** High, around 70-85%\n",
    "   - **Best overall** for this problem\n",
    "\n",
    "4. **ε = 0.3 (High exploration):**\n",
    "   - **Initial learning:** Fastest to try all arms\n",
    "   - **Long-term:** Wastes time on suboptimal arms\n",
    "   - **Optimal action rate:** Limited by exploration (max ~70% since 30% random)\n",
    "   - **Total reward:** Lower than ε=0.1 despite finding optimal arm\n",
    "\n",
    "**Key insight:** There's a sweet spot for ε (usually 0.05-0.1) that balances exploration and exploitation optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Compare Epsilon-Greedy vs UCB - Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running comparison experiments...\")\n",
    "print(\"  Epsilon-Greedy (ε=0.1)...\")\n",
    "\n",
    "# Run epsilon-greedy\n",
    "rewards_egreedy, optimal_egreedy = run_multiple_experiments(\n",
    "    EpsilonGreedyAgent,\n",
    "    {'n_arms': 10, 'epsilon': 0.1},\n",
    "    n_experiments=100,\n",
    "    n_steps=1000,\n",
    "    n_arms=10\n",
    ")\n",
    "\n",
    "print(\"  UCB (c=2.0)...\")\n",
    "\n",
    "# Run UCB\n",
    "rewards_ucb, optimal_ucb = run_multiple_experiments(\n",
    "    UCBAgent,\n",
    "    {'n_arms': 10, 'c': 2.0},\n",
    "    n_experiments=100,\n",
    "    n_steps=1000,\n",
    "    n_arms=10\n",
    ")\n",
    "\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Average reward\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_egreedy, label='Epsilon-Greedy (ε=0.1)', linewidth=2, alpha=0.8)\n",
    "plt.plot(rewards_ucb, label='UCB (c=2.0)', linewidth=2, alpha=0.8)\n",
    "plt.xlabel('Steps', fontsize=12)\n",
    "plt.ylabel('Average Reward', fontsize=12)\n",
    "plt.title('Epsilon-Greedy vs UCB: Average Reward', fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Optimal action rate\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(optimal_egreedy * 100, label='Epsilon-Greedy (ε=0.1)', linewidth=2, alpha=0.8)\n",
    "plt.plot(optimal_ucb * 100, label='UCB (c=2.0)', linewidth=2, alpha=0.8)\n",
    "plt.xlabel('Steps', fontsize=12)\n",
    "plt.ylabel('% Optimal Action', fontsize=12)\n",
    "plt.title('Epsilon-Greedy vs UCB: Optimal Action Rate', fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nFinal Performance (last 100 steps average):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Epsilon-Greedy (ε=0.1):\")\n",
    "print(f\"  Average Reward:      {np.mean(rewards_egreedy[-100:]):.4f}\")\n",
    "print(f\"  Optimal Action Rate: {np.mean(optimal_egreedy[-100:])*100:.2f}%\")\n",
    "print(f\"\\nUCB (c=2.0):\")\n",
    "print(f\"  Average Reward:      {np.mean(rewards_ucb[-100:]):.4f}\")\n",
    "print(f\"  Optimal Action Rate: {np.mean(optimal_ucb[-100:])*100:.2f}%\")\n",
    "\n",
    "# Additional analysis: convergence speed\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Convergence Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Find when each algorithm reaches 50% optimal action rate\n",
    "threshold = 0.5\n",
    "egreedy_50 = np.argmax(optimal_egreedy > threshold)\n",
    "ucb_50 = np.argmax(optimal_ucb > threshold)\n",
    "\n",
    "print(f\"Steps to reach 50% optimal action rate:\")\n",
    "print(f\"  Epsilon-Greedy: {egreedy_50} steps\")\n",
    "print(f\"  UCB:           {ucb_50} steps\")\n",
    "\n",
    "# Cumulative regret (difference from optimal)\n",
    "# Note: This is approximate since we don't know the true optimal value\n",
    "# We'll use the best average reward seen as a proxy\n",
    "best_reward = max(np.max(rewards_egreedy), np.max(rewards_ucb))\n",
    "regret_egreedy = np.cumsum(best_reward - rewards_egreedy)\n",
    "regret_ucb = np.cumsum(best_reward - rewards_ucb)\n",
    "\n",
    "print(f\"\\nCumulative Regret (lower is better):\")\n",
    "print(f\"  Epsilon-Greedy: {regret_egreedy[-1]:.2f}\")\n",
    "print(f\"  UCB:           {regret_ucb[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Analysis: Epsilon-Greedy vs UCB\n",
    "\n",
    "**1. Initial Learning Phase (Steps 0-100):**\n",
    "\n",
    "- **UCB typically converges faster initially**\n",
    "  - Systematically explores all arms first (N=0 handling)\n",
    "  - Directed exploration based on uncertainty\n",
    "  - Quickly identifies promising arms\n",
    "\n",
    "- **Epsilon-Greedy starts slower**\n",
    "  - Random exploration means some arms tried multiple times before others\n",
    "  - Can get \"unlucky\" and focus on suboptimal arms early\n",
    "  - More variable initial performance\n",
    "\n",
    "**2. Mid-term Performance (Steps 100-500):**\n",
    "\n",
    "- **UCB's advantage becomes clear**\n",
    "  - Exploration naturally decreases as uncertainty reduces\n",
    "  - Focuses more on promising arms\n",
    "  - Better cumulative reward\n",
    "\n",
    "- **Epsilon-Greedy catches up**\n",
    "  - More samples → better estimates\n",
    "  - But still wastes 10% of actions on random exploration\n",
    "\n",
    "**3. Long-term Performance (Steps 500-1000):**\n",
    "\n",
    "- **UCB maintains edge**\n",
    "  - Adaptive exploration means almost always picking optimal arm\n",
    "  - Can reach >95% optimal action rate\n",
    "  - Lower cumulative regret\n",
    "\n",
    "- **Epsilon-Greedy plateaus**\n",
    "  - Limited to ~90% optimal (10% random)\n",
    "  - Continues wasting actions on exploration\n",
    "  - But simple and reliable\n",
    "\n",
    "**4. Key Trade-offs:**\n",
    "\n",
    "| Aspect | Epsilon-Greedy | UCB |\n",
    "|--------|---------------|-----|\n",
    "| **Simplicity** | Very simple | More complex |\n",
    "| **Hyperparameters** | One (ε), easy to tune | One (c), less intuitive |\n",
    "| **Computational cost** | Very low | Slightly higher (sqrt, log) |\n",
    "| **Theoretical guarantees** | Weak | Strong (logarithmic regret) |\n",
    "| **Performance** | Good | Better |\n",
    "| **Adaptivity** | Fixed exploration | Adaptive exploration |\n",
    "| **Robustness** | Very robust | Sensitive to c choice |\n",
    "\n",
    "**5. When to use each:**\n",
    "\n",
    "**Use Epsilon-Greedy when:**\n",
    "- Simplicity is paramount\n",
    "- Computational resources are very limited\n",
    "- You need interpretable behavior\n",
    "- Problem is non-stationary (changing over time)\n",
    "- Quick prototyping\n",
    "\n",
    "**Use UCB when:**\n",
    "- Maximizing long-term reward is critical\n",
    "- Problem is stationary (fixed arm distributions)\n",
    "- You want theoretical guarantees\n",
    "- You can afford slightly more computation\n",
    "- Optimal performance matters more than simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Analysis Questions - Sample Answers\n",
    "\n",
    "### 1. Effect of epsilon:\n",
    "\n",
    "**ε = 0 (Pure Greedy):**\n",
    "- Gets stuck on the first arm that appears good\n",
    "- No exploration means it can't discover better arms\n",
    "- Performance is highly dependent on initial random pulls\n",
    "- Can work if the initial estimate happens to be correct, but usually fails\n",
    "- Example: If first pull of arm 3 gives high reward by chance, agent keeps pulling arm 3 even if arm 7 is actually better\n",
    "\n",
    "**ε = 0.3 (Too Large):**\n",
    "- Explores too much, wasting 30% of actions on random pulls\n",
    "- Finds optimal arm quickly but doesn't exploit it enough\n",
    "- Average reward is limited because of constant random exploration\n",
    "- Optimal action rate can never exceed 70% (since 30% is random)\n",
    "- Good for initial learning, bad for long-term performance\n",
    "\n",
    "**Best ε:**\n",
    "- For this problem, ε = 0.1 works best\n",
    "- Provides enough exploration to find optimal arm\n",
    "- Doesn't waste too many actions on suboptimal arms\n",
    "- Achieves good balance between learning and earning\n",
    "- General rule: Start with ε ∈ [0.05, 0.15] and adjust based on problem\n",
    "\n",
    "### 2. Epsilon-Greedy vs UCB:\n",
    "\n",
    "**Initial Convergence:**\n",
    "- UCB converges faster in the first 50-100 steps\n",
    "- Reason: UCB systematically tries each arm first, then focuses on promising ones\n",
    "- Epsilon-greedy's random exploration is less efficient initially\n",
    "- UCB's directed exploration (based on uncertainty) is more effective\n",
    "\n",
    "**Long-term Performance:**\n",
    "- UCB achieves better long-term performance\n",
    "- Reaches higher optimal action rates (>95% vs ~90%)\n",
    "- Lower cumulative regret\n",
    "- Epsilon-greedy is limited by its fixed exploration rate\n",
    "- UCB's adaptive exploration means it \"learns to exploit\" better over time\n",
    "\n",
    "**When to Prefer UCB:**\n",
    "- When you need optimal performance and have many steps\n",
    "- Stationary problems (arm distributions don't change)\n",
    "- When theoretical guarantees matter\n",
    "- When you can afford slight computational overhead\n",
    "- Applications: Clinical trials, A/B testing with long run times, resource allocation\n",
    "\n",
    "### 3. Trade-offs:\n",
    "\n",
    "**Epsilon-Greedy Advantages:**\n",
    "- **Simplicity:** Only 3 lines of code for action selection\n",
    "- **Intuitive:** Everyone understands \"try random things 10% of the time\"\n",
    "- **Robust:** Works reasonably well across many problems\n",
    "- **Easy tuning:** ε is interpretable and easy to adjust\n",
    "- **Computational:** Extremely fast (just argmax and random)\n",
    "- **Non-stationary:** Fixed exploration helps track changing environments\n",
    "\n",
    "**UCB Advantages:**\n",
    "- **Performance:** Provably optimal with logarithmic regret\n",
    "- **Adaptive:** Automatically reduces exploration over time\n",
    "- **Intelligent exploration:** Explores uncertain arms, not random arms\n",
    "- **No waste:** Eventually almost always picks optimal arm\n",
    "- **Theoretical guarantees:** Mathematically proven to work\n",
    "\n",
    "**Scenarios for Epsilon-Greedy:**\n",
    "1. **Quick prototyping:** Need to test RL quickly\n",
    "2. **Teaching/Learning:** Introducing RL concepts\n",
    "3. **Non-stationary environments:** User preferences change over time\n",
    "4. **Embedded systems:** Limited computation (no sqrt/log operations)\n",
    "5. **Short-term problems:** Only 100-200 steps, simplicity matters more\n",
    "6. **Unknown problem characteristics:** Safe default that usually works\n",
    "\n",
    "**Real-world example:** A recommendation system for news articles\n",
    "- News relevance changes quickly (non-stationary)\n",
    "- Need fast decisions (millions of users)\n",
    "- Epsilon-greedy is better: simple, fast, handles changing interests\n",
    "- UCB's stationarity assumption is violated, and computation matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge Solutions\n",
    "\n",
    "### 1. Decaying Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecayingEpsilonGreedyAgent:\n",
    "    \"\"\"Epsilon-greedy with decaying exploration rate.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, epsilon_start: float = 1.0, \n",
    "                 epsilon_end: float = 0.01, decay_rate: float = 0.995):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        self.Q = np.zeros(n_arms)\n",
    "        self.N = np.zeros(n_arms)\n",
    "    \n",
    "    def select_action(self) -> int:\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            action = np.argmax(self.Q)\n",
    "        \n",
    "        # Decay epsilon after each action\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.decay_rate)\n",
    "        return action\n",
    "    \n",
    "    def update(self, action: int, reward: float):\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] += (reward - self.Q[action]) / self.N[action]\n",
    "\n",
    "print(\"Testing Decaying Epsilon-Greedy:\")\n",
    "agent = DecayingEpsilonGreedyAgent(n_arms=10, epsilon_start=1.0, epsilon_end=0.01, decay_rate=0.995)\n",
    "epsilons = []\n",
    "for _ in range(1000):\n",
    "    epsilons.append(agent.epsilon)\n",
    "    action = agent.select_action()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(epsilons)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title('Epsilon Decay Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Starting epsilon: {epsilons[0]:.3f}\")\n",
    "print(f\"Final epsilon: {epsilons[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Decaying Epsilon Works:**\n",
    "- Early on: High exploration to discover good arms\n",
    "- Later: Low exploration to maximize rewards\n",
    "- Mimics UCB's adaptive exploration behavior\n",
    "- Common in deep RL (e.g., DQN uses this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optimistic Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare pessimistic (Q=0) vs optimistic (Q=5) initialization\n",
    "print(\"Comparing different initial Q-values...\")\n",
    "\n",
    "# Pessimistic\n",
    "rewards_pessimistic, optimal_pessimistic = run_multiple_experiments(\n",
    "    EpsilonGreedyAgent,\n",
    "    {'n_arms': 10, 'epsilon': 0, 'initial_value': 0.0},  # Greedy with Q=0\n",
    "    n_experiments=50,\n",
    "    n_steps=1000\n",
    ")\n",
    "\n",
    "# Optimistic\n",
    "rewards_optimistic, optimal_optimistic = run_multiple_experiments(\n",
    "    EpsilonGreedyAgent,\n",
    "    {'n_arms': 10, 'epsilon': 0, 'initial_value': 5.0},  # Greedy with Q=5\n",
    "    n_experiments=50,\n",
    "    n_steps=1000\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(optimal_pessimistic * 100, label='Pessimistic (Q=0)', alpha=0.7)\n",
    "plt.plot(optimal_optimistic * 100, label='Optimistic (Q=5)', alpha=0.7)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('% Optimal Action')\n",
    "plt.title('Effect of Initial Q-values on Greedy Agent')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final optimal rate (Q=0): {np.mean(optimal_pessimistic[-100:])*100:.1f}%\")\n",
    "print(f\"Final optimal rate (Q=5): {np.mean(optimal_optimistic[-100:])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimistic Initial Values Insight:**\n",
    "- Setting Q = 5 (optimistic) encourages exploration even with ε = 0\n",
    "- Agent thinks all arms are great initially\n",
    "- When it tries an arm and gets reward < 5, it's \"disappointed\"\n",
    "- Tries other arms looking for the \"5 reward\" it expects\n",
    "- Simple way to encourage exploration without explicit randomness\n",
    "- Works well for stationary problems\n",
    "- Problem: Need to know a good optimistic value (problem-dependent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways from Week 1\n",
    "\n",
    "1. **RL is about learning through interaction**\n",
    "   - No labels, only reward signals\n",
    "   - Must balance exploration and exploitation\n",
    "\n",
    "2. **Multi-Armed Bandits are fundamental**\n",
    "   - Simplest RL problem (no state transitions)\n",
    "   - Core challenge: exploration vs exploitation\n",
    "   - Foundation for more complex RL\n",
    "\n",
    "3. **Multiple valid strategies exist**\n",
    "   - Epsilon-greedy: Simple, robust, good baseline\n",
    "   - UCB: Principled, adaptive, optimal guarantees\n",
    "   - No single \"best\" method for all problems\n",
    "\n",
    "4. **Implementation matters**\n",
    "   - Incremental updates are efficient\n",
    "   - Proper testing requires multiple experiments\n",
    "   - Visualization reveals algorithm behavior\n",
    "\n",
    "5. **Next steps: Adding state**\n",
    "   - Bandits have no state (each action independent)\n",
    "   - Next week: MDPs add state transitions\n",
    "   - Actions affect future states and rewards\n",
    "   - Need to plan ahead, not just maximize immediate reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
